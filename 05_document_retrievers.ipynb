{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a32b216-a2b4-4714-a2ad-ee7c2b62da65",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Custom Document Retrievers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334d167-0724-4032-9b92-d4c8798e7789",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Introducing Chroma DB</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Chroma DB é um serviço open-source de vector stores baseado no SQLite.        \n",
    "        </li>\n",
    "        <li>\n",
    "            A classe `Chroma`, do módulo `langchain.vectorstores.chroma`, criará sozinha uma base de dados. Informe apenas a lista de chunks de seu arquivo, a classe do modelo de Embeddings e o nome do diretório que conterá a DB.\n",
    "        </li>\n",
    "        <li>\n",
    "            Apenas tenha cuidado, porque rodar o método from_documents múltiplas vezes ocasionará no append de duplicatas na DB!\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='img/05_chroma.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f6ef5-758c-476f-9419-6d94fd65a8dd",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Query de Similaridades</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Uma vez criada a base, você pode informar uma string e requisitar os k chunks mais similares a ela.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='img/05_similarity_query.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65e227-0dcd-4087-84ba-70b7c8ec36ad",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Building a Retrieval Chain</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Retrieval Chains (RetrievalQA) são uma modalidade de chain especializada em queries em vectorstores. Elas obtêm o embedding mais similar ao da pergunta do usuário e inserem o chunk dentro do texto do prompt.\n",
    "        </li>\n",
    "        <li>\n",
    "            O prompt formatado é, logo em seguida, encaminhado ao LLM para a geração da resposta.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='img/05_retrieval_qa.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c50d6-84b2-4b96-8925-50062b22daa9",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> RetrievalQA</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O RetrievalQA deve ser instanciado com um LLM e um retriver. Defina o argumento `chain_type` como \"stuff\".\n",
    "        </li>\n",
    "        <li>\n",
    "            O retriver pode ser criado com uma instanciação diferente da classe `Chroma`, que recebe apenas o diretório da vectorstore e o modelo de embeddings usado.\n",
    "        </li>\n",
    "        <li>\n",
    "            Para invocar a chain, use o método `run` com a pergunta.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='img/05_retrieval_qa_chain.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c336d2de-f53b-4045-b00e-cf85b57e4256",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> What is a Retriever?</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Os Retrievers são classes responsáveis por fazerem as queries em vectorstores. Nós os criamos quando invocamos o método `as_retriever` na classe de nossa DB.\n",
    "        </li>\n",
    "        <li>\n",
    "            No backend, os retrievers rodam o método `_get_relevant_documents` para resgatar os chunks mais similares ao da pergunta.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7076384-d3ab-44e6-aaf7-f489d957cd2f",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Chain Type Stuff</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Quando definimos o argumento `chain_type='stuff'`, em `RetrievalQA`, mandamos o LangChain concatenar os documentos mais similares na System Message.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='img/05_stuff.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479f4b9-5523-496c-9524-00bafdc43095",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Understanding Refine, MapReduce, and MapRerank </h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> Nessa aula, vamos aprender os outros tipos de chain types disponíveis no LangChain.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42feb424-980d-4e48-91cc-3799fd4bda2b",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Chain Types</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de92c32-68c2-42f9-813c-4d6a64f68eca",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> MapReduce</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Uma vez feitos os RAG's, encaminhamos os resultados da query ao LLM para que esse retorne a porção dos documentos que se relacionam com a pergunta do usuário.\n",
    "        </li>\n",
    "        <li>\n",
    "            Tendo os sub-fragmentos, os encaminhamos ao LLM para que ele finalmente responda a pergunta do usuário (à moda stuff).  \n",
    "        </li>\n",
    "        <li>\n",
    "            A desvantagem principal desse chain type é o alto consumo do LLM - as requisições por sub-fragmentos são feitas uma por uma. Ademais, existe o risco de alucinação do modelo, em caso do sub-fragmento não tiver qualquer relação com a pergunta do usuário.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55cf3d4-93bf-468a-aff1-2a0c45ac112a",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> MapRerank</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Nessa modalidade, o LLM é instruído a receber um sub-fragmento por vez e elaborar uma resposta à pergunta do usuário. Ela deve vir junto com um score do quão relacionada ela foi com a indagação.\n",
    "        </li>\n",
    "        <li>\n",
    "            Finalmente, pedimos para o modelo responder a pergunta do usuário, considerando o sub-fragmento com maior pontuação.\n",
    "        </li>\n",
    "        <li>\n",
    "            No caso de alucinações do LLM - quando o sub-fragmento não tiver qualquer relação com a pergunta -, existe o risco de esse texto criado ser considerado o mais relacionado com a pergunta. \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa9838-7def-45f0-9858-a67356a0f44f",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Refine</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Tem funcionamento iterativo. O modelo primeiro elabora uma resposta com base no chunk de cosseno maior. Logo em seguida, o desafiamos a enriquecer a resposta, com base em outro chunk de alta similaridade. \n",
    "        </li>\n",
    "        <li>\n",
    "            Esse processo é usado até o último sub-fragmento do RAG. É um método bastante propenso a erros!\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d14d3-f0cb-4332-8dc3-2ca4c29659e0",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Removing Duplicate Documents </h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Quando fazemos uma consulta em uma base de dados vetorial, não desejamos também que os chunks retornados contenham a mesma informação, dita de maneiras diferentes. No final das contas, acabaríamos aumentando nosso prompt com fatos repetidos, encarecendo a requisição à Open AI desnecessariamente. \n",
    "        </li>\n",
    "        <li>\n",
    "            O que poderíamos fazer seria comparar os embeddings dos chunks entre si e eliminar vetores muito similares a outros.\n",
    "        </li>\n",
    "        <li>\n",
    "            O LangChain possui a classe EmbeddingsRedundantFilter para isso, mas ela faz uma outra requisição ao modelo de embedding para conversão dos textos, ao invés de só consultar o Chroma DB. Portanto, vamos ter que criar um Retriever customizado que ferá devidamete a query na DB.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da8562-6771-4557-8b44-8585c8f2fea0",
   "metadata": {},
   "source": [
    "<p style='color:red'> Vi aula Removing Duplicate Documents; Aula 42</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

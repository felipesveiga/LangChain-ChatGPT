{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686ffb7d-0ba6-488f-866b-08b52595ed36",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Extending LangChain</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266c078-5306-4bf9-9774-748e7ea67ac9",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Testing the Streaming Setup</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Uma vez implantada a nossa chain com suporte a streaming, repararemos num erro do Flask que afirma que a Thread criada não tem acesso ao cotexto de nossa aplicação.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='img/11_thread_context.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "        <li style='margin-top:20px'>\n",
    "            O contexto do site contém metadados importantes para a sua execução.\n",
    "        </li>\n",
    "        <li>\n",
    "            Felizmente, o Flask tem uma variável (a `current_app`), que possibilita que nós invoquemos o contexto da aplicação dentro do contexto de um objeto particular.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4d3c0-7795-48f7-984d-26cd6d19fc93",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='margin-top:20px'>\n",
    "        <img src='img/11_current_app.png'>\n",
    "            <figcaption> Implementação do `current_app` na classe de nossa `StreamableChain`.</figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda88e82-99b7-4c47-bb70-7ae79a3ef8e3",
   "metadata": {},
   "source": [
    "<!-- <h3 style='font-size:30px;font-style:italic'> </h3> -->\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Uma vez implantada essa correção, repararemos um outro comportamento pouco esperado de nossa aplicação: o modelo parece que não consegue entender nossas questões!\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='img/11_strange_behaviour.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8689e5b6-0714-417f-bc67-6a9d5c4f8bba",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Here's the Issue</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Devemos nos lembrar que a chain do projeto é, na verdade, um wrapper de outras duas chains.\n",
    "        </li>\n",
    "        <li>\n",
    "            O problema apontado está acontecendo, porque a segunda chain está consumindo o callback da primeira chain.\n",
    "        </li>\n",
    "        <li>\n",
    "            Como essa instância do callback já foi utilizada na readaptação da pergunta, ela fica incapaz de receber os tokens emitidos pela real resposta do LLM. No final das contas, recebemos como output a pergunta reformulada.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f63f1-fce8-46ee-9f84-10a8d2a98770",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Isolating the Handler</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            O LangChain nos permite passar callbacks como argumentos de diversas classes/operações.\n",
    "        </li>\n",
    "        <li>\n",
    "            As ligações desse callback dependerão de onde ele for instanciado. Por exemplo, quando o informamos na instanciação de um LLM, ele será exclusivamente vinculado a esse.     \n",
    "        </li>\n",
    "        <li>\n",
    "            No entanto, caso o passemos na invocação de uma chain - como é o caso de nosso projeto -, ele será ligado a todos os componentes que essa classe abrange. No caso de nossa StreamingConversationalRetrievalChain, tanto a chain de sintetização de textos, quanto a de resposta receberão o mesmo StreamingCallback, sendo por isso que elas o compartilham no uso da aplicação.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa896523-89af-4339-ab89-2d20658f6300",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Nossa Solução</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Como não podemos informar o callback fora da invocação da chain - questões do projeto -, faremos uma breve customização do nosso programa. \n",
    "        </li>\n",
    "        <li>\n",
    "            Nas classes de LLM, definiremos o argumento `streaming` como False na chain de sintetização, e True na de output. Assim, instruiremos nosso callback a inserir dados na Queue, apenas caso `streaming=True`.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a77efe-2ea9-404d-9dff-00dd97d13606",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Streaming Complete!</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Para fazer o que propusemos, precisamos informar agora duas classes de LLM para nossa chain. Felizmente, a `ConversationalRetrievalChain` admite receber uma classe que será de uso exclusivo para a condensação das perguntas.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='img/11_condense_llm.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb10644-d646-4474-be14-275da11a94f0",
   "metadata": {},
   "source": [
    "<p style='color:red'> Iniciei Aula 114; Aula 114 (2:00); Iniciar edição do callback</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

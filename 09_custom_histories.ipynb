{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0cb4abc-c77e-44a3-bbd3-418701708200",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Custom Message Histories</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Esta seção será dedicada à montagem da interface de chat do nosso site. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70deb419-0f06-45d9-9b3f-8672dbb66c57",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Understanding the Apps Requirements</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            A montagem da interface envolverá certos desafios, como a persistência das mensagens trocadas nos diversos chats tidos. Além disso, vamos ter que garantir que a nossa classe de RAG resgate apenas os chunks referentes ao pdf escolhido. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56547f-4ec2-4d13-bb46-8f95a78cb79f",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Persistent Message Storage</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            O desafio da filtragem dos embeddings pode ser resolvido passando um dicionário como parâmetro de nosso objeto retriever. No nosso caso, deveremos passar o id do pdf como nosso argumento. \n",
    "            <center style='margin-top:20px'>\n",
    "                <img src='img/09_rag_filter.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27808f-588f-4128-a1fe-ab2515cdf7d2",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Chat History Persistence</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Dado o alto volume de conversas esperado em nossa plataforma, não seria apropriado armazená-las todas em memória (ChatMemoryBuffer). \n",
    "        </li>\n",
    "        <li>\n",
    "            Ao invés disso, guardaremos todos os históricos numa DB SQLite. Quando um chat for continuado, faremos uma query que alocará somente o histórico do chat específico à nossa memória!\n",
    "        </li>\n",
    "        <li>\n",
    "            Essa tarefa demandará que nós criemos um objeto de memória customizado.\n",
    "            <center style='margin-top:20px'>\n",
    "                <img src='img/09_memory_persistence.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0cfee0-651b-4cdc-8189-13cefe1552d2",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Introducing the Conversational Retrieval Chain</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Apesar de o LangChain ter a classe `RetrievalQA`, essa não tem memória embutida. E mesmo se tentássemos acrescentar isso, a vetorização poderia estar sujeita a algumas imperfeições.\n",
    "        </li>\n",
    "        <li>\n",
    "            Foi considerando isso que o LangChain desenvolveu a ConversationalRetrievalChain. Nela, pedimos ao LLM para pôr um resumo da conversa até então, antes do texto da nova pergunta do usuário.\n",
    "        </li>\n",
    "        <li>\n",
    "            Tendo esse novo texto em mãos, o vetorizamos e fazemos um RAG na Vector DB para assim fazermos a requisição oficial ao LLM. Por fim, a resposta deste e a pergunta do usuário - e não o resumo do modelo!- são alocados à memória.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30939d-c645-4c7c-83f2-c1576c8a1b7d",
   "metadata": {},
   "source": [
    "<p style='color:red'> Vi Aula 92; Ver Aula 93</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac249761-6f1d-4a8f-b9b6-604140eaa075",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Streaming Text Generation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee31f4-ac58-4fa5-91fb-df541eb06b42",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>  Streaming Text Generation</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Atualmente, o usuário de nosso site deverá aguardar a geração de todo o texto para que esse apareça em sua tela. Isso não oferece a melhor UX, porque o cliente poderá ficar muito tempo aguardando a criação do texto. \n",
    "        </li>\n",
    "        <li>\n",
    "            Nesta seção, vamos fazer com que a geração da resposta se pareça com a do Chat-GPT, em que o usuário verá o texto sendo construído diante de seus olhos.\n",
    "        </li>\n",
    "        <li>\n",
    "            O segredo disso é configurarmos o LLM para lançar a resposta com chunks.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d31017-a908-493e-a059-6ddc29563055",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Experimenting with a Streaming Language Model</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Para habilitar respostas streaming, podemos definir o argumento `streaming=True` do nosso modelo de linguagem.\n",
    "        </li>\n",
    "        <li>\n",
    "            No entanto, o sucesso ou não desse tipo de output também depende da maneira como invocamos nossa IA. A depender de como acionamos o modelo (`Classe(message)`, `Classe.invoke(message)`), o streaming pode ser prejudicado. Isso porque as diferentes funções de invocação lidam com a maneira como a classe do modelo tratará a resposta da API.\n",
    "        </li>\n",
    "        <li>\n",
    "            Para garantir que o output será dado como um streaming, ative a requisição com o método `stream`. Seu resultado será um objeto generator com os pedaços das respostas.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c482db-66f0-463e-b9ab-da3488e23b13",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Chains Don't Want to Stream</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            As classes de chains impõem um empecilho a mais na sucessão dos streamings. Isso porque elas esperam obter as respostas completas para poder definir algumas de suas propriedades, e atualizar o seu histórico de mensagens.\n",
    "        </li>\n",
    "        <li>\n",
    "            Apesar de elas terem suporte para uma função `stream`, este produz apenas um generator de um único item. Portanto, teremos que criar uma chain customizada!\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eabd60c-24d2-40b6-b3bf-10b3a701d530",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Receiving Chunks with a Callback</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Nossa solução para o problema do streaming envolverá a criação de um callback. Nós o definiremos com uma função `on_llm_new_token`, que será acionada a cada vez que um novo token for recebido pela API do modelo.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3030dc-812d-4fb9-94fe-26c1b3a800c2",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Extending a LLM Chain</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Começaremos a montagem de nossa chain customizada. Basicamente, ela será herdeira da `LLMChain`, com o método `stream` customizado à nossa necessidade.\n",
    "        </li>\n",
    "        <li>\n",
    "            O maior desafio será integrá-la com o nosso callback.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6eb86-7ec8-471b-8002-d2056b05228b",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Adding a Queue for Communication</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            O instrutor achou melhor armazenarmos cada chunk retornado pela API em uma Queue - provavelmente FIFO. Dessa forma, o método `stream` somente deverá consultá-la e extrair cada chunk tido.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5c14c-c477-4ef9-af01-1f3b558ecfa3",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Solving the Slow Chain</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Códigos em Python são executados de maneira sequencial. Essa característica da linguagem pode ser danosa ao nosso projeto, porque gostaríamos que os chunks fossem publicados, conforme o modelo os gera.\n",
    "        </li>\n",
    "        <li>\n",
    "            Para isso, teríamos que paralelizar a execução do modelo, e a iteração que consulta a Queue. Nesse caso, temos a opção de recorrer à classe `threading.Thread`, que receberá função que será executada em paralelo com o código que vem logo após sua definição.\n",
    "            <center style='margin-top:20px'> \n",
    "                <img src='img/10_thread.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551019f3-4dc7-4ea9-90fe-2b33588336ba",
   "metadata": {},
   "source": [
    "<p style='color:red'> Vi Aulas 104, 105 e 106; Aula 107</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac249761-6f1d-4a8f-b9b6-604140eaa075",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Streaming Text Generation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee31f4-ac58-4fa5-91fb-df541eb06b42",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>  Streaming Text Generation</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Atualmente, o usuário de nosso site deverá aguardar a geração de todo o texto para que esse apareça em sua tela. Isso não oferece a melhor UX, porque o cliente poderá ficar muito tempo aguardando a criação do texto. \n",
    "        </li>\n",
    "        <li>\n",
    "            Nesta seção, vamos fazer com que a geração da resposta se pareça com a do Chat-GPT, em que o usuário verá o texto sendo construído diante de seus olhos.\n",
    "        </li>\n",
    "        <li>\n",
    "            O segredo disso é configurarmos o LLM para lançar a resposta com chunks.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d31017-a908-493e-a059-6ddc29563055",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Experimenting with a Streaming Language Model</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            Para habilitar respostas streaming, podemos definir o argumento `streaming=True` do nosso modelo de linguagem.\n",
    "        </li>\n",
    "        <li>\n",
    "            No entanto, o sucesso ou não desse tipo de output também depende da maneira como invocamos nossa IA. A depender como acionamos o modelo (`Classe(message)`, `Classe.invoke(message)`), o streaming pode ser prejudicado. Isso porque as diferentes funções de invocação lidam com a maneira como a classe do modelo tratará a resposta da API.\n",
    "        </li>\n",
    "        <li>\n",
    "            Para garantir que o output será dado como um streaming, ative a requisição com o método `stream`. Seu resultado será um objeto generator com os pedaços das respostas.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c482db-66f0-463e-b9ab-da3488e23b13",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Chains Don't Want to Stream</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>   \n",
    "            As classes de chains impõem um empecilho a mais na sucessão dos streamings. Isso porque elas esperam obter as respostas completas para poder definir algumas de suas propriedades, e atualizar o seu histórico de mensagens.\n",
    "        </li>\n",
    "        <li>\n",
    "            Apesar de elas terem suporte para uma função `stream`, este produz apenas um generator de um único item. Portanto, teremos que criar uma chain customizada!\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551019f3-4dc7-4ea9-90fe-2b33588336ba",
   "metadata": {},
   "source": [
    "<p style='color:red'> Vi Aulas 99 e 100; Aula 101</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
